Waa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']
   
    ### START CODE HERE ###
    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (â‰ˆ2 lines)
    #for gradient in [dWax, dWaa, dWya, db, dby]:
    for gradient in gradients.keys():
        print(gradient)
        
        #print(gradients[gradient])
        gradients[gradient]=np.clip(gradients[gradient],-maxValue,maxValue)
    ### END CODE HERE ###
    
    #gradients = {"dWaa": dWaa, "dWax": dWax, "dWya": dWya, "db": db, "dby": dby}


while (idx != newline_character and counter != 50):
        
        # Step 2: Forward propagate x using the equations (1), (2) and (3)
        a = np.tanh(np.matmul(Waa,a_prev)+np.matmul(Wax,x)+b)
        z = np.tanh(np.matmul(Wya,a)+by)
        y = softmax(z)
        print(y)
        # for grading purposes
        np.random.seed(counter+seed) 
        
        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y
        idx = np.random.choice(range(vocab_size),p=y)

        # Append the index to "indices"
        indices.append(index)
        
        # Step 4: Overwrite the input character as the one corresponding to the sampled index.
        x = y
        x[idx] = 1
        
        # Update "a_prev" to be "a"
        a_prev = a
        
        # for grading purposes
        seed += 1
        counter +=1
        
